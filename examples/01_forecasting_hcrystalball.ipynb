{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with sktime\n",
    "\n",
    "### Reduction: from forecasting to regression\n",
    "\n",
    "sktime provides a meta-estimator for this approach, which is:\n",
    "\n",
    "* **modular** and **compatible with scikit-learn**, so that we can easily apply any scikit-learn regressor to solve our forecasting problem,\n",
    "* **tuneable**, allowing us to tune hyper-parameters like the window length or strategy to generate forecasts\n",
    "* **adaptive**, in the sense that it adapts the scikit-learn's estimator interface to that of a forecaster, making sure that we can tune and properly evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.all import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = load_airline()\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=36)\n",
    "print(y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hcrystalball.wrappers import SarimaxWrapper\n",
    "from hcrystalball.wrappers import ExponentialSmoothingWrapper\n",
    "from hcrystalball.wrappers import get_sklearn_wrapper\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hcrystalball.feature_extraction import SeasonalityTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SarimaxWrapper(init_with_autoarima=True, autoarima_dict={\"m\":12})\n",
    "# model = ExponentialSmoothingWrapper(trend=\"add\", seasonal=\"multiplicative\", seasonal_periods=12)\n",
    "# model = get_sklearn_wrapper(RandomForestRegressor, n_estimators=100)\n",
    "# model = pipeline = Pipeline([\n",
    "#     ('seasonality', SeasonalityTransformer(freq='Y')),\n",
    "#     ('model', get_sklearn_wrapper(RandomForestRegressor))\n",
    "# ])\n",
    "# model = get_sklearn_wrapper(ElasticNet)\n",
    "model = pipeline = Pipeline([\n",
    "    ('seasonality', SeasonalityTransformer(freq='Y')),\n",
    "    ('model', get_sklearn_wrapper(ElasticNet))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.base._sktime import OptionalForecastingHorizonMixin\n",
    "from sktime.forecasting.base._sktime import BaseSktimeForecaster\n",
    "from sktime.forecasting.base._base import DEFAULT_ALPHA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class HCrystalBallForecaster(OptionalForecastingHorizonMixin, BaseSktimeForecaster):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self._is_fitted = False\n",
    "        \n",
    "    def fit(self, y_train, fh=None, X_train=None): \n",
    "        self._set_y_X(y_train, X_train)\n",
    "        self._set_fh(fh)   \n",
    "        \n",
    "        y_train, X_train = self._adapt_fit_data(y_train, X_train)\n",
    "                \n",
    "        self.model.fit(X=X_train, y=y_train)        \n",
    "        self._is_fitted = True\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, fh=None, X=None, return_pred_int=False, alpha=DEFAULT_ALPHA):\n",
    "        if return_pred_int:\n",
    "            self._check_model_consistent_with_pred_int(alpha)\n",
    "            \n",
    "        self.check_is_fitted()\n",
    "        self._set_fh(fh)\n",
    "        \n",
    "        X = self._adapt_predict_data(X)\n",
    "                \n",
    "        preds = self.model.predict(X=X)\n",
    "        \n",
    "        return self._convert_predictions(preds)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def _adapt_fit_data(y_train, X_train): \n",
    "        X_train = X_train or pd.DataFrame()\n",
    "        \n",
    "        if type(y_train.index) in (pd.PeriodIndex, pd.DatetimeIndex):\n",
    "            y = pd.Series(data=y_train.values,\n",
    "                          index=y_train.index.to_timestamp() if type(y_train.index) is pd.PeriodIndex else y_train.index, \n",
    "                )\n",
    "            X = pd.DataFrame(index=y.index)            \n",
    "            if type(X_train.index) == type(X.index):\n",
    "                X = X.merge(X_train, left_index=True, right_index=True)\n",
    "            else:\n",
    "                # different types, let's use X_train values with the datetime index inferred from y_train\n",
    "                X = pd.DataFrame(data=X_train.values, index=X.index)\n",
    "                    \n",
    "        elif type(X_train.index) in (pd.PeriodIndex, pd.DatetimeIndex):\n",
    "            #TODO check for X(2020-09-01, 2020-09-02, 2020-09-10), while y(2020-09-01 ... 2020-09-10)\n",
    "            X = pd.DataFrame(\n",
    "                    data=X_train.values, \n",
    "                    index=X_train.index.to_timestamp() if type(X_train.index) is pd.PeriodIndex else X_train.index\n",
    "            )\n",
    "            y_train = pd.Series(data=y_train.values, index=X.index)\n",
    "        else:\n",
    "            raise ValueError(\"At least one of y_train or X_train must have Period or DateTime index. \"\n",
    "                             f\"You provided {type(X_train.index)} for X_train.index and {type(y_train.index)} for y_train.index\")\n",
    "\n",
    "        return y_train, X\n",
    "        \n",
    "    def _adapt_predict_data(self, X):\n",
    "        _X = pd.DataFrame(index=self._fh.to_absolute(self._cutoff))\n",
    "        _X.index = _X.index.to_timestamp()\n",
    "        X = X or pd.DataFrame()\n",
    "        \n",
    "        if type(X.index) == type(_X.index):\n",
    "            return _X.merge(X, left_index=True, right_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame(data=X.values, index=_X.index)\n",
    "            \n",
    "    @staticmethod\n",
    "    def _convert_predictions(preds):\n",
    "        preds = preds.iloc[:,0]\n",
    "        preds.index = preds.index.to_period()\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    #TODO: update per model/once the support is there for all models\n",
    "    def _check_model_consistent_with_pred_int(alpha):\n",
    "        raise NotImplemented(\"Full support for confidence intervals is not implemented.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = HCrystalBallForecaster(model=model)\n",
    "fh = np.arange(len(y_test)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster.fit(y_train, \n",
    "#                fh=fh\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forecaster.predict(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical forecasters\n",
    "\n",
    "sktime has a number of statistical forecasting algorithms, based on implementations in statsmodels. For example, to use exponential smoothing with an additive trend component and multiplicative seasonality, we can write the following.\n",
    "\n",
    "Note that since this is monthly data, the seasonal periodicity (sp), or the number of periods per year, is 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecaster = HCrystalBallForecaster(model = ExponentialSmoothingWrapper(trend=\"add\", seasonal=\"multiplicative\", seasonal_periods=12))\n",
    "forecaster = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The exponential smoothing of state space model can also be automated similar\n",
    " to the [ets](https://www.rdocumentation.org/packages/forecast/versions/8.13/topics/ets) function in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = HCrystalBallForecaster(model = ExponentialSmoothingWrapper(trend=\"add\", seasonal=\"multiplicative\", seasonal_periods=12))\n",
    "# forecaster = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from sktime.forecasting.ets import AutoETS\n",
    "# forecaster = AutoETS(auto=True, sp=12, n_jobs=-1)\n",
    "# forecaster.fit(y_train)\n",
    "# y_pred = forecaster.predict(fh)\n",
    "# plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "# smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common model is the ARIMA model. In sktime, we interface [pmdarima](https://github.com/alkaline-ml/pmdarima), a package for automatically selecting the best ARIMA model. This since searches over a number of possible model parametrisations, it may take a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecaster = HCrystalBallForecaster(model = SarimaxWrapper(init_with_autoarima=True, autoarima_dict={\"m\":12}))\n",
    "forecaster = AutoARIMA(sp=12, suppress_warnings=True)\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = HCrystalBallForecaster(model = SarimaxWrapper(init_with_autoarima=True, autoarima_dict={\"m\":12}))\n",
    "# forecaster = AutoARIMA(sp=12, suppress_warnings=True)\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composite model building\n",
    "\n",
    "sktime provides a modular API for composite model building for forecasting.\n",
    "\n",
    "### Ensembling\n",
    "Like scikit-learn, sktime provides a meta-forecaster to ensemble multiple forecasting algorithms. For example, we can combine different variants of exponential smoothing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = EnsembleForecaster([\n",
    "    (\"ses\", ExponentialSmoothing(seasonal=\"multiplicative\", sp=12)),\n",
    "    (\"holt\", ExponentialSmoothing(trend=\"add\", damped=False, seasonal=\"multiplicative\", sp=12)),\n",
    "    (\"damped\", ExponentialSmoothing(trend=\"add\", damped=True, seasonal=\"multiplicative\", sp=12))\n",
    "])\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = EnsembleForecaster([\n",
    "    (\"ses\", HCrystalBallForecaster(ExponentialSmoothingWrapper(seasonal=\"multiplicative\", seasonal_periods=12))),\n",
    "    (\"holt\", HCrystalBallForecaster(ExponentialSmoothingWrapper(trend=\"add\", damped=False, seasonal=\"multiplicative\", seasonal_periods=12))),\n",
    "    (\"damped\", HCrystalBallForecaster(ExponentialSmoothingWrapper(trend=\"add\", damped=True, seasonal=\"multiplicative\", seasonal_periods=12)))\n",
    "])\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "In the `ReducedRegressionForecaster`, both the `window_length` and `strategy` arguments are hyper-parameters which we may want to optimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = ReducedRegressionForecaster(regressor=regressor, window_length=15, strategy=\"recursive\")\n",
    "param_grid = {\"window_length\": [5, 10, 15]}\n",
    "\n",
    "# we fit the forecaster on the initial window, and then use temporal cross-validation to find the optimal parameter\n",
    "cv = SlidingWindowSplitter(initial_window=int(len(y_train) * 0.5))\n",
    "gscv = ForecastingGridSearchCV(forecaster, cv=cv, param_grid=param_grid)\n",
    "gscv.fit(y_train)\n",
    "y_pred = gscv.predict(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn's `GridSearchCV`, we can tune regressors imported from scikit-learn, in addition to tuning `window_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# tuning the 'n_estimator' hyperparameter of RandomForestRegressor from scikit-learn\n",
    "regressor_param_grid = {\"n_estimators\": [100, 200, 300]}\n",
    "forecaster_param_grid = {\"window_length\": [5,10,15,20,25]}\n",
    "\n",
    "# create a tunnable regressor with GridSearchCV\n",
    "regressor = GridSearchCV(RandomForestRegressor(), param_grid=regressor_param_grid)\n",
    "forecaster = ReducedRegressionForecaster(regressor, window_length=15, strategy=\"recursive\")\n",
    "\n",
    "cv = SlidingWindowSplitter(initial_window=int(len(y_train) * 0.5))\n",
    "gscv = ForecastingGridSearchCV(forecaster, cv=cv, param_grid=forecaster_param_grid)\n",
    "\n",
    "gscv.fit(y_train)\n",
    "y_pred = gscv.predict(fh)\n",
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gscv.best_params_, gscv.best_forecaster_.regressor_.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access performance on a particular metric during tuning, we can use the `scoring` argument of `ForecastingGridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv = ForecastingGridSearchCV(forecaster, cv=cv, param_grid=forecaster_param_grid, scoring=sMAPE())\n",
    "gscv.fit(y_train)\n",
    "pd.DataFrame(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detrending\n",
    "Note that so far the reduction approach above does not take any seasonal or trend into account, but we can easily specify a pipeline which first detrends the data.\n",
    "\n",
    "sktime provides a generic detrender, a transformer which uses any forecaster and returns the in-sample residuals of the forecaster's predicted values. For example, to remove the linear trend of a time series, we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liner detrending\n",
    "forecaster = PolynomialTrendForecaster(degree=1)\n",
    "transformer = Detrender(forecaster=forecaster)\n",
    "yt = transformer.fit_transform(y_train)\n",
    "\n",
    "# internally, the Detrender uses the in-sample predictions of the PolynomialTrendForecaster\n",
    "forecaster = PolynomialTrendForecaster(degree=1)\n",
    "fh_ins = -np.arange(len(y_train)) # in-sample forecasting horizon\n",
    "y_pred = forecaster.fit(y_train).predict(fh=fh_ins)\n",
    "\n",
    "plot_series(y_train, y_pred, yt, labels=[\"y_train\", \"fitted linear trend\", \"residuals\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining\n",
    "\n",
    "Let's use the detrender in a pipeline together with de-seasonalisation. Note that in forecasting, when we apply data transformations before fitting, we need to apply the inverse transformation to the predicted values. For this purpose, we provide the following pipeline class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = TransformedTargetForecaster([\n",
    "    (\"deseasonalise\", Deseasonalizer(model=\"multiplicative\", sp=12)),\n",
    "    (\"detrend\", Detrender(forecaster=PolynomialTrendForecaster(degree=1))),\n",
    "    (\"forecast\", ReducedRegressionForecaster(regressor=regressor, window_length=12, strategy=\"recursive\"))\n",
    "])\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"]);\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we could try again to optimise the hyper-parameters of components of the pipeline.\n",
    "\n",
    "Below we discuss two other aspects of forecasting: online learning, where we want to dynamically update forecasts as new data comes in, and prediction intervals, which allow us to quantify the uncertainty of our forecasts.\n",
    "\n",
    "## Dynamic forecasts\n",
    "\n",
    "For model evaluation, we sometimes want to evaluate multiple forecasts, using temporal cross-validation with a sliding window over the test data. For this purpose, all forecasters in sktime have a `update_predict` method. Here we make repeated single-step ahead forecasts over the test set.\n",
    "\n",
    "Note that the forecasting task is changed: while we still make 36 predictions, we do not predict 36 steps ahead, but instead make 36 single-step-ahead predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = NaiveForecaster(strategy=\"last\")\n",
    "forecaster.fit(y_train)\n",
    "cv = SlidingWindowSplitter(fh=1)\n",
    "y_pred = forecaster.update_predict(y_test, cv)\n",
    "smape_loss(y_test, y_pred)\n",
    "plot_series(y_train, y_test, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single update, you can use the `update` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction intervals\n",
    "So far, we've only looked at point forecasts. In many cases, we're also interested in prediction intervals. sktime's interface support prediction intervals, but we haven't implemented them for all algorithms yet.\n",
    "\n",
    "Here, we use the Theta forecasting algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = ThetaForecaster(sp=12)\n",
    "forecaster.fit(y_train)\n",
    "alpha = 0.05  # 95% prediction intervals\n",
    "y_pred, pred_ints = forecaster.predict(fh, return_pred_int=True, alpha=alpha)\n",
    "smape_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])\n",
    "ax.fill_between(ax.get_lines()[-1].get_xdata(), pred_ints[\"lower\"], pred_ints[\"upper\"],\n",
    "                alpha=0.2, color=ax.get_lines()[-1].get_c(), label=f\"{1 - alpha}% prediction intervals\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "As we have seen, in order to make forecasts, we need to first specify (or build) a model, then fit it to the training data, and finally call predict to generate forecasts for the given forecasting horizon.\n",
    "\n",
    "* sktime comes with several forecasting algorithms (or forecasters) and tools for composite model building. All forecaster share a common interface. Forecasters are trained on a single series of data and make forecasts for the provided forecasting horizon.\n",
    "\n",
    "* sktime has a number of statistical forecasting algorithms, based on implementations in statsmodels. For example, to use exponential smoothing with an additive trend component and multiplicative seasonality, we can write the following.\n",
    "\n",
    "\n",
    "## Useful resources\n",
    "* For more details, take a look at [our paper on forecasting with sktime](https://arxiv.org/abs/2005.08067) in which we discuss the forecasting API in more detail and use it to replicate and extend the M4 study.\n",
    "* For a good introduction to forecasting, see [Hyndman, Rob J., and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018](https://otexts.com/fpp2/).\n",
    "* For comparative benchmarking studies/forecasting competitions, see the [M4 competition](https://www.sciencedirect.com/science/article/pii/S0169207019301128) and the currently running [M5 competition](https://www.kaggle.com/c/m5-forecasting-accuracy/overview)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
